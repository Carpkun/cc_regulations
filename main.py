import streamlit as st
import os
from langchain_community.document_loaders import PDFPlumberLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document
from langchain_upstage import UpstageEmbeddings
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_google_genai import ChatGoogleGenerativeAI

# ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
# Streamlit: ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ ì¸í„°í˜ì´ìŠ¤ë¥¼ ë§Œë“¤ê¸° ìœ„í•´ ì‚¬ìš©í•©ë‹ˆë‹¤.
# os: íŒŒì¼ ê²½ë¡œ ì‘ì—…ì„ ìœ„í•´ ì‚¬ìš©í•©ë‹ˆë‹¤.
# langchain ë° ê´€ë ¨ ëª¨ë“ˆ: ë¬¸ì„œ ë¡œë“œ, í…ìŠ¤íŠ¸ ë¶„í• , ì„ë² ë”© ìƒì„±, ê²€ìƒ‰ê¸° êµ¬ì„± ë“±ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
os.environ["GOOGLE_API_KEY"] = st.secrets["general"]["google_api_key"]
os.environ["UPSTAGE_API_KEY"] = st.secrets["general"]["upstage_api_key"]

# í”„ë¡œì íŠ¸ ë¡œê¹… ì„¤ì •
from langchain_teddynote import logging
logging.langsmith("CC_REGULATION_Chatbot")

# Streamlit ì›¹ ì•±ì˜ ì œëª© ì„¤ì •
st.title("ì¶˜ì²œë¬¸í™”ì› ì œê·œì • GPT ğŸ’¬")

# ì‚¬ìš©ìì˜ ëŒ€í™” ìƒíƒœë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
if "messages" not in st.session_state:
    st.session_state["messages"] = []  # ì‚¬ìš©ìì˜ ëŒ€í™” ë©”ì‹œì§€ë¥¼ ì €ì¥í•˜ê¸° ìœ„í•œ ë¦¬ìŠ¤íŠ¸
    st.session_state["db_initialized"] = False  # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ìƒíƒœë¥¼ ì¶”ì í•˜ëŠ” í”Œë˜ê·¸

# ì‚¬ì´ë“œë°”ì—ì„œ ì´ˆê¸°í™” ë²„íŠ¼ì„ ìƒì„±í•©ë‹ˆë‹¤.
with st.sidebar:
    clear_btn = st.button("ëŒ€í™” ì´ˆê¸°í™”")

# ì±—ë´‡ì´ ì°¸ì¡°í•  PDF íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
pdf_files = [
    "data/regulations.pdf"
]

# í…ìŠ¤íŠ¸ ë¶„í• ê¸°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì‘ì€ ì²­í¬ë¡œ ë‚˜ëˆ„ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=120)

# ì„ë² ë”© ëª¨ë¸ì„ ì„¤ì •í•©ë‹ˆë‹¤. í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.
embeddings = UpstageEmbeddings(model="solar-embedding-1-large-passage")

# ë²¡í„° ì €ì¥ì†Œ(FAISS)ì˜ ê²½ë¡œì™€ ì´ë¦„ì„ ì„¤ì •í•©ë‹ˆë‹¤.
faiss_db_path = "faiss_db"
faiss_index_name = "faiss_index"

# ë²¡í„° ì €ì¥ì†Œë¥¼ ì´ˆê¸°í™”í•˜ê³  êµ¬ì¶•í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
def initialize_db():
    if not os.path.exists(faiss_db_path):  # ì €ì¥ì†Œê°€ ì—†ëŠ” ê²½ìš°
        documents = []
        for pdf_file in pdf_files:
            loader = PDFPlumberLoader(pdf_file)  # PDF íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.
            split_docs = loader.load_and_split(text_splitter)  # PDFë¥¼ ì‘ì€ ì²­í¬ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.
            documents.extend([Document(page_content=str(page)) for page in split_docs])

        # FAISS DBë¥¼ ìƒì„±í•˜ê³  ë¡œì»¬ì— ì €ì¥í•©ë‹ˆë‹¤.
        db = FAISS.from_documents(documents=documents, embedding=embeddings)
        db.save_local(folder_path=faiss_db_path, index_name=faiss_index_name)
    else:
        # ê¸°ì¡´ì— ì €ì¥ëœ FAISS DBë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
        db = FAISS.load_local(
            folder_path=faiss_db_path,
            index_name=faiss_index_name,
            embeddings=embeddings,
            allow_dangerous_deserialization=True,
        )
    st.session_state["db_initialized"] = True  # ë°ì´í„°ë² ì´ìŠ¤ê°€ ì´ˆê¸°í™”ë¨ì„ í‘œì‹œí•©ë‹ˆë‹¤.
    return db

# ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë¡œë“œí•˜ê±°ë‚˜ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
if not st.session_state["db_initialized"]:
    loaded_db = initialize_db()  # ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
else:
    loaded_db = FAISS.load_local(  # ì´ë¯¸ ì´ˆê¸°í™”ëœ ê²½ìš° ê¸°ì¡´ DBë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
        folder_path=faiss_db_path,
        index_name=faiss_index_name,
        embeddings=embeddings,
        allow_dangerous_deserialization=True,
    )

# ê²€ìƒ‰ê¸°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.
retriever = loaded_db.as_retriever()

# ì´ì „ ëŒ€í™” ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
def print_messages():
    for chat_message in st.session_state["messages"]:
        st.chat_message(chat_message["role"]).write(chat_message["content"])

# ìƒˆë¡œìš´ ë©”ì‹œì§€ë¥¼ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
def add_message(role, message):
    st.session_state["messages"].append({"role": role, "content": message})

# ì§ˆì˜ì‘ë‹µ ì²´ì¸ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
def create_chain():
    prompt = PromptTemplate.from_template(
        """You are an assistant for question-answering tasks. 
        Use the following pieces of retrieved context to answer the question. 
        If you don't know the answer, just say that you don't know. 
        Answer in Korean.
        Make sure the output is all the same size characters.
        Never enclose your answer in parentheses.
        Be sure to include your source and page numbers in your answer.
        Please follow the rules below when attributing sources.
        - "regulations.pdf" : "ì¶˜ì²œë¬¸í™”ì› ì œê·œì •ì§‘"
            
        #Example Format:
        (brief summary of the answer)
        (table)
        (detailed answer to the question)
    
        **ì¶œì²˜**
        - (page source and page number)

        #Question: 
        {question}

        #Context: 
        {context} 

        #Answer:"""
    )

    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0)

    chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    return chain

# ì´ˆê¸°í™” ë²„íŠ¼ì´ ëˆŒë ¸ì„ ê²½ìš°
if clear_btn:
    st.session_state["messages"] = []

# ì´ì „ ëŒ€í™” ê¸°ë¡ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
print_messages()

# ì‚¬ìš©ì ì…ë ¥ì„ ë°›ìŠµë‹ˆë‹¤.
user_input = st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!")

# ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ ì…ë ¥í–ˆì„ ê²½ìš°
if user_input:
    # ì‚¬ìš©ì ì…ë ¥ì„ ê¸°ë¡í•˜ê³  í™”ë©´ì— ì¶œë ¥í•©ë‹ˆë‹¤.
    st.chat_message("user").write(user_input)
    add_message("user", user_input)

    # ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    chain = create_chain()

    # ê²€ìƒ‰ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ ë¬¸ì„œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    context_docs = retriever.invoke(user_input)

    if context_docs:
        # ê´€ë ¨ ë¬¸ì„œì—ì„œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê²°í•©í•©ë‹ˆë‹¤.
        context = "\n".join([doc.page_content for doc in context_docs])
        
        # ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
        response = chain.invoke({"question": user_input, "context": context})

        # ë‹µë³€ì„ í™”ë©´ì— ì¶œë ¥í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤.
        st.chat_message("assistant").write(response)
        add_message("assistant", response)
    else:
        # ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆì„ ê²½ìš°ì˜ ê¸°ë³¸ ë‹µë³€
        response = "í•´ë‹¹ ë¬¸ì„œì—ì„œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì‹œë„í•´ ì£¼ì„¸ìš”."
        st.chat_message("assistant").write(response)
        add_message("assistant", response)